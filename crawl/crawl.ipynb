{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from crawlbase import CrawlingAPI\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(response):\n",
    "    try:\n",
    "        # Parse the HTML content using Beautiful Soup\n",
    "        soup = BeautifulSoup(response['body'], 'html.parser')\n",
    "\n",
    "        # Extract the title of the Wikipedia page\n",
    "        title = soup.find('h1', id='firstHeading').get_text(strip=True)\n",
    "\n",
    "        # Extract the main content of the Wikipedia page\n",
    "        content_div = soup.find('div', id='mw-content-text')\n",
    "        elements = content_div.find_all(['p', 'h2', 'h3']) if content_div else []\n",
    "        content = []\n",
    "\n",
    "        for element in elements:\n",
    "            # Handle headings (h2 and h3)\n",
    "            if element.name == 'h2':\n",
    "                heading_text = element.get_text(strip=True)\n",
    "                content.append(f\"== {heading_text} ==\")\n",
    "            elif element.name == 'h3':\n",
    "                heading_text = element.get_text(strip=True)\n",
    "                content.append(f\"=== {heading_text} ===\")\n",
    "            # Handle paragraph content\n",
    "            elif element.name == 'p':\n",
    "                for sup in element.find_all('sup'):\n",
    "                    sup.decompose()  # Remove footnotes and references\n",
    "\n",
    "                processed_paragraph = []\n",
    "                for sub_element in element.descendants:\n",
    "                    if sub_element.name == 'a':\n",
    "                        processed_paragraph.append(f\"[{sub_element.get_text(strip=True)}]\")\n",
    "                    elif sub_element.string:\n",
    "                        processed_paragraph.append(sub_element.string.strip())\n",
    "\n",
    "                content.append(' '.join(processed_paragraph))\n",
    "\n",
    "        # Join all content lines with newline\n",
    "        content = '\\n'.join(content)\n",
    "\n",
    "        return {\n",
    "            'title': title,\n",
    "            'content': content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(page_url, api_token):\n",
    "    # Initialize the CrawlingAPI object with your token\n",
    "    api = CrawlingAPI({'token': api_token})\n",
    "\n",
    "    # Get the page content\n",
    "    response = api.get(page_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response['status_code'] == 200:\n",
    "        # Scraped data\n",
    "        return scrape_data(response)\n",
    "    else:\n",
    "        print(f\"Error: {response}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(scraped_data_list, output_file):\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "            fieldnames = ['keyword', 'title', 'content']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write data rows\n",
    "            for data in scraped_data_list:\n",
    "                writer.writerow(data)\n",
    "\n",
    "        print(f\"Data has been saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving data to file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_keywords(api_token, keywords_file, output_file):\n",
    "    # Read keywords from file\n",
    "    with open(keywords_file, 'r', encoding='utf-8') as file:\n",
    "        keywords = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "    scraped_data_list = []\n",
    "\n",
    "    cnt=0\n",
    "    # Crawl data for each keyword\n",
    "    for keyword in keywords:\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{keyword.replace(' ', '_')}\"\n",
    "        print(f\"Crawling data for: {keyword}\")\n",
    "        scraped_data = crawl(page_url, api_token)\n",
    "\n",
    "        if scraped_data:\n",
    "            scraped_data['keyword'] = keyword\n",
    "            scraped_data_list.append(scraped_data)\n",
    "\n",
    "    # Save all scraped data to a single CSV file\n",
    "    save_to_csv(scraped_data_list, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Crawling data for: Anarchism\n",
      "1\n",
      "Crawling data for: Computer\n",
      "Data has been saved to scraped_data.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify your Crawlbase API token\n",
    "    api_token = 'RX8U0YDT1DNPn70uji3IJQ'\n",
    "\n",
    "    # File containing keywords (one keyword per line)\n",
    "    keywords_file = 'keywords.txt'\n",
    "\n",
    "    # Output CSV file\n",
    "    output_file = 'scraped_data.csv'\n",
    "\n",
    "    # Crawl data for all keywords\n",
    "    crawl_keywords(api_token, keywords_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
